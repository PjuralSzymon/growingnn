{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home Page","text":"<p>The growingnn project introduces an innovative algorithm for data-driven neural network model construction. This algorithm comprises two fundamental components: the first component focuses on weight adjustment, while the second component acts as an orchestrator, launching a guided procedure to dynamically change the network architecture. This architectural modification occurs at regular intervals, specifically every \\(K\\) epochs, and is driven by the outcome of a Monte Carlo tree search. The algorithm's core, outlined in the accompanying research paper, leverages the principles of Stochastic Gradient Descent (SGD) without relying on advanced tools commonly used in neural network training.</p>"},{"location":"#algorithm-overview","title":"Algorithm Overview","text":""},{"location":"#weight-adjustment-component","title":"Weight Adjustment Component","text":"<p>The first component of the algorithm is dedicated to weight adjustment. It operates within the framework of Stochastic Gradient Descent (SGD), a foundational optimization algorithm for training neural networks. The simplicity of this approach makes it suitable for educational settings, emphasizing fundamental machine learning principles.</p>"},{"location":"#orchestrator-and-network-architecture-modification","title":"Orchestrator and Network Architecture Modification","text":"<p>The second component, the orchestrator, plays a crucial role in initiating a procedure to dynamically change the network architecture. This change occurs systematically at predefined intervals, specifically every \\(K\\) epochs. The decision-making process for architectural changes is facilitated by a guided Monte Carlo tree search. This sophisticated mechanism ensures that architectural modifications are well-informed and contribute to the overall improvement of the neural network model.</p>"},{"location":"#implementation-details","title":"Implementation Details","text":""},{"location":"#model-structure","title":"Model Structure","text":"<p>The model is the main structure that stores layers as nodes in a directed graph. It operates based on layer identifiers, treating each layer as an independent structure that contains information about incoming and outgoing connections. The default starting structure is a simple graph with an input and output layer connected by a single connection. In each generation, the algorithm has the flexibility to add new layers or remove existing ones. As the structure grows, each layer gains more incoming and outgoing connections.</p>"},{"location":"#propagation-phase","title":"Propagation Phase","text":"<p>During the propagation phase, each layer waits until it receives signals from all input layers. Once these signals are received, they are averaged, processed, and propagated through all outgoing connections. This iterative process allows the neural network to dynamically adapt its architecture based on the evolving data and training requirements.</p>"},{"location":"#results-and-testing","title":"Results and Testing","text":"<p>The proposed algorithm has undergone rigorous testing, particularly in visual pattern classification problems. The results have consistently demonstrated high levels of satisfaction, showcasing the efficacy of the dynamic architecture learning approach in enhancing model performance.</p> <p><pre><code>x_train, x_test, y_train, y_test, labels = data_reader.read_mnist_data(mnist_path, 0.9)\ngnn.trainer.train(\n    x_train = x_train, \n    y_train = y_train, \n    x_test = x_test,\n    y_test = y_test,\n    labels = labels,\n    input_paths = 1,\n    path = \"./result\", \n    model_name = \"GNN_model\",\n    epochs = 10, \n    generations = 10,\n    input_size = 28 * 28, \n    hidden_size = 28 * 28, \n    output_size = 10, \n    input_shape = (28, 28, 1), \n    kernel_size = 3, \n    depth = 2\n)\n</code></pre> This code trains a simple network on the MNIST dataset</p>"},{"location":"#structure","title":"Structure","text":"<p>History of resulting optimal structure created for MNIST dataset:</p> <ol> <li> <p></p> </li> <li> <p></p> </li> <li> <p></p> </li> <li> <p></p> </li> </ol> <p>History of training for mnist and fmnist dataset: </p>"},{"location":"#credits","title":"Credits","text":"<p>Szymon Swiderski Agnieszka Jastrzebska</p>"},{"location":"#disclosure","title":"Disclosure","text":"<p>This is the first beta version of the growingnn package. We are not liable for the accuracy of the program\u2019s output nor actions performed based upon it.</p> <p>For more in-depth information on the algorithm, its implementation, and testing results, refer to the accompanying research paper. The provided Python source code is a valuable resource for understanding and implementing the presented method. Feel free to explore, contribute, and adapt it for your specific needs.</p>"},{"location":"page_actions/","title":"Actions","text":""},{"location":"page_actions/#overview","title":"Overview","text":"<p>The <code>Action</code> module is responsible for managing structural modifications to a neural network model. It defines different actions that can be performed on a model, such as adding or removing layers. This module enables dynamic modifications to the network structure, enhancing the adaptability of the learning process.</p> <p>In our approach, we utilize a structured search method where different actions are generated and evaluated. Each action affects the model's topology, and by executing these actions in a controlled manner, we enable adaptive learning. Inspired by Schaul et al. \\cite{DBLP:journals/corr/Smith15a}, we allow progressive modifications to the structure, ensuring that drastic changes do not disrupt the learning process.</p>"},{"location":"page_actions/#action-class-hierarchy","title":"Action Class Hierarchy","text":""},{"location":"page_actions/#action","title":"<code>Action</code>","text":"<p>The base class for all actions. Each action must implement:</p> <ul> <li><code>execute(Model)</code>: Applies the action to the given model.</li> <li><code>can_be_influenced(by_action)</code>: Determines if an action is affected by another action.</li> <li><code>generate_all_actions(Model)</code>: Generates all possible instances of the action for a given model.</li> </ul>"},{"location":"page_actions/#add_seq_layer","title":"<code>Add_Seq_Layer</code>","text":"<p>Adds a sequential layer between two existing layers. The layer type is determined based on the model's current state.</p> <ul> <li>Execution: <code>Model.add_norm_layer(layer1, layer2, layer_type)</code></li> <li>Influence: A delete action on either layer1 or layer2 affects this action.</li> <li>Generation: Iterates over sequence connections to propose new layers.</li> </ul>"},{"location":"page_actions/#example","title":"Example:","text":"<pre><code>action = Add_Seq_Layer([layer_1, layer_2, Layer_Type.EYE])\naction.execute(model)\n</code></pre>"},{"location":"page_actions/#add_res_layer","title":"<code>Add_Res_Layer</code>","text":"<p>Adds a residual connection between two layers, allowing gradient flow across multiple layers.</p> <ul> <li>Execution: <code>Model.add_res_layer(layer1, layer2, layer_type)</code></li> <li>Influence: A delete action on either layer1 or layer2 affects this action.</li> <li>Generation: Identifies child-parent connections and proposes residual links.</li> </ul>"},{"location":"page_actions/#example_1","title":"Example:","text":"<pre><code>action = Add_Res_Layer([layer_1, layer_2, Layer_Type.RANDOM])\naction.execute(model)\n</code></pre>"},{"location":"page_actions/#del_layer","title":"<code>Del_Layer</code>","text":"<p>Removes a layer from the model, updating all associated connections.</p> <ul> <li>Execution: <code>Model.remove_layer(layer_id)</code></li> <li>Influence: Does not get influenced by other actions.</li> <li>Generation: Proposes deletion for each hidden layer.</li> </ul>"},{"location":"page_actions/#example_2","title":"Example:","text":"<pre><code>action = Del_Layer(layer_id)\naction.execute(model)\n</code></pre>"},{"location":"page_actions/#add_seq_conv_layer","title":"<code>Add_Seq_Conv_Layer</code>","text":"<p>Adds a sequential convolutional layer between two existing layers.</p> <ul> <li>Execution: <code>Model.add_conv_norm_layer(layer1, layer2)</code></li> <li>Influence: A delete action on either layer1 or layer2 affects this action.</li> <li>Generation: Identifies potential convolutional connections.</li> </ul>"},{"location":"page_actions/#example_3","title":"Example:","text":"<pre><code>action = Add_Seq_Conv_Layer([conv_layer_1, conv_layer_2])\naction.execute(model)\n</code></pre>"},{"location":"page_actions/#add_res_conv_layer","title":"<code>Add_Res_Conv_Layer</code>","text":"<p>Adds a residual convolutional layer between two existing layers.</p> <ul> <li>Execution: <code>Model.add_conv_res_layer(layer1, layer2)</code></li> <li>Influence: A delete action on either layer1 or layer2 affects this action.</li> <li>Generation: Identifies convolutional child-parent connections.</li> </ul>"},{"location":"page_actions/#example_4","title":"Example:","text":"<pre><code>action = Add_Res_Conv_Layer([conv_layer_1, conv_layer_2])\naction.execute(model)\n</code></pre>"},{"location":"page_actions/#empty","title":"<code>Empty</code>","text":"<p>A placeholder action that does nothing, useful for maintaining structure.</p> <ul> <li>Execution: No effect on the model.</li> <li>Influence: Not influenced by any action.</li> <li>Generation: Always returns a single empty action.</li> </ul>"},{"location":"page_actions/#example_5","title":"Example:","text":"<pre><code>action = Empty(None)\naction.execute(model)\n</code></pre>"},{"location":"page_actions/#summary","title":"Summary","text":"<p>This module enables dynamic modifications to a neural network structure, supporting structural search and optimization techniques. The approach is designed to maintain stability while iteratively refining the network topology for improved learning performance.</p>"},{"location":"page_examples/","title":"Examples","text":"<p>This page provides examples of how to use the <code>growingnn</code> library for training and evaluating neural network models. The examples progress from simple to more complex use cases.</p>"},{"location":"page_examples/#basic-examples","title":"Basic Examples","text":""},{"location":"page_examples/#1-basic-model-creation-and-training-with-sgd","title":"1. Basic Model Creation and Training with SGD","text":"<p>The simplest way to create and train a model using Stochastic Gradient Descent (SGD) is as follows:</p> <pre><code>import growingnn as gnn\nimport numpy as np\n\n# Define model parameters\ninput_size = 20\noutput_size = 2\nhidden_layers = 1\n\n# Create a model\nmodel = gnn.structure.Model(\n    input_size, input_size, output_size,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    hidden_layers,\n    gnn.optimizers.SGDOptimizer()\n)\n\n# Generate random data\nx_train = np.random.rand(input_size, input_size)\ny_train = np.random.randint(2, size=(input_size,))\n\n# Set learning rate scheduler\nlr_scheduler = gnn.structure.LearningRateScheduler(\n    gnn.structure.LearningRateScheduler.PROGRESIVE, 0.03, 0.8\n)\n\n# Train model\naccuracy, _ = model.gradient_descent(x_train, y_train, epochs=5, lr_scheduler=lr_scheduler)\n\n# Print accuracy\nprint(f\"Training accuracy: {accuracy}\")\n</code></pre>"},{"location":"page_examples/#2-training-with-adam-optimizer","title":"2. Training with Adam Optimizer","text":"<p>The following example trains a model using the Adam optimizer:</p> <pre><code>import growingnn as gnn\nimport numpy as np\n\n# Create a model with Adam optimizer\nmodel = gnn.structure.Model(\n    20, 20, 2,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    1,\n    gnn.optimizers.AdamOptimizer()\n)\n\n# Generate training data\nx_train = np.random.rand(20, 20)\ny_train = np.random.randint(2, size=(20,))\n\n# Define learning rate scheduler\nlr_scheduler = gnn.structure.LearningRateScheduler(\n    gnn.structure.LearningRateScheduler.PROGRESIVE, 0.03, 0.8\n)\n\n# Train model\naccuracy, _ = model.gradient_descent(x_train, y_train, epochs=5, lr_scheduler=lr_scheduler)\n\n# Print accuracy\nprint(f\"Training accuracy: {accuracy}\")\n</code></pre>"},{"location":"page_examples/#3-adding-and-removing-residual-layers","title":"3. Adding and Removing Residual Layers","text":"<p>The following example demonstrates how to add and remove residual layers dynamically:</p> <pre><code>import growingnn as gnn\nimport numpy as np\n\n# Create a model\nmodel = gnn.structure.Model(\n    20, 20, 2,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    1,\n    gnn.optimizers.SGDOptimizer()\n)\n\n# Add a residual layer\nlayer_id = model.add_res_layer('init_0', 1)\nprint(f\"Added residual layer with ID: {layer_id}\")\n\n# Remove the layer\nmodel.remove_layer(layer_id)\nprint(\"Residual layer removed successfully.\")\n</code></pre>"},{"location":"page_examples/#4-forward-propagation-example","title":"4. Forward Propagation Example","text":"<p>Forward propagation can be tested using a simple example:</p> <pre><code>import growingnn as gnn\nimport numpy as np\n\n# Create a model\nmodel = gnn.structure.Model(\n    10, 10, 3,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    1,\n    gnn.optimizers.SGDOptimizer()\n)\n\n# Generate input data\ninput_data = np.random.rand(10, 10)\n\n# Perform forward propagation\noutput = model.forward_prop(input_data)\n\n# Print output shape\nprint(f\"Output shape: {output.shape}\")\n</code></pre>"},{"location":"page_examples/#5-training-a-convolutional-model-with-sgd","title":"5. Training a Convolutional Model with SGD","text":"<p>If using convolutional networks, you can set up and train the model like this:</p> <pre><code>import growingnn as gnn\nimport numpy as np\n\n# Create a convolutional model\nmodel = gnn.structure.Model(\n    20, 20, 20,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    1,\n    gnn.optimizers.SGDOptimizer()\n)\n\n# Set convolution mode\nmodel.set_convolution_mode((20, 20, 1), 20, 1)\nmodel.add_res_layer('init_0', 1)\n\n# Generate input data\nx_train = np.random.random((20, 20, 20, 1))\n\n# Forward propagation test\noutput = model.forward_prop(x_train)\nprint(f\"Output shape: {output.shape}\")\n</code></pre>"},{"location":"page_examples/#6-comparing-adam-vs-sgd-performance","title":"6. Comparing Adam vs SGD Performance","text":"<p>This example trains models using both Adam and SGD optimizers and compares their accuracy:</p> <pre><code>import growingnn as gnn\nimport numpy as np\n\n# Generate dataset\nx_train = np.random.random((10, 20))\ny_train = np.random.randint(3, size=(20,))\n\n# Train model with Adam optimizer\nadam_model = gnn.structure.Model(10, 10, 3,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    1,\n    gnn.optimizers.AdamOptimizer()\n)\nadam_model.gradient_descent(x_train, y_train, epochs=5)\nacc_adam = gnn.Model.get_accuracy(gnn.Model.get_predictions(adam_model.forward_prop(x_train)), y_train)\n\n# Train model with SGD optimizer\nsgd_model = gnn.structure.Model(10, 10, 3,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    1,\n    gnn.optimizers.SGDOptimizer()\n)\nsgd_model.gradient_descent(x_train, y_train, epochs=5)\nacc_sgd = gnn.Model.get_accuracy(gnn.Model.get_predictions(sgd_model.forward_prop(x_train)), y_train)\n\n# Compare accuracy\nprint(f\"Adam accuracy: {acc_adam}, SGD accuracy: {acc_sgd}\")\n</code></pre>"},{"location":"page_examples/#7-saving-and-loading-models","title":"7. Saving and Loading Models","text":"<p>The following example demonstrates how to save and load models:</p> <pre><code>import growingnn as gnn\nimport numpy as np\n\n# Create a model\nmodel = gnn.structure.Model(\n    3, 3, 1,\n    gnn.structure.Loss.multiclass_cross_entropy,\n    gnn.structure.Activations.Sigmoid,\n    1\n)\n\n# Generate data\nx_train = np.random.rand(3, 3)\n\n# Perform forward propagation\noutput_before = model.forward_prop(x_train)\n\n# Save the model\ngnn.Storage.saveModel(model, \"model.json\")\n\n# Load the model\nloaded_model = gnn.Storage.loadModel(\"model.json\")\n\n# Perform forward propagation again\noutput_after = loaded_model.forward_prop(x_train)\n\n# Ensure outputs are identical\nassert np.allclose(output_before, output_after), \"Model outputs differ after loading.\"\nprint(\"Model successfully saved and loaded.\")\n</code></pre> <p>These examples cover the fundamental usage of <code>growingnn</code>, progressing from simple training to more advanced features like convolution, residual layers, and model storage.</p>"},{"location":"page_examples/#examples-with-training-a-model-with-trainer","title":"Examples with training a model with trainer","text":""},{"location":"page_examples/#1-training-a-dense-network","title":"1. Training a Dense Network","text":"<p>This example demonstrates training a dense network with a small dataset:</p> <pre><code>import numpy as np\n\n# Generate synthetic data\nx_train = np.random.random((10, 20))\ny_train = np.random.randint(3, size=(20,))\nx_test = np.random.random((10, 10))\ny_test = np.random.randint(3, size=(10,))\n\n# Train model\ntrained_model = gnn.trainer.train(\n    x_train=x_train,\n    y_train=y_train,\n    x_test=x_test,\n    y_test=y_test,\n    labels=range(3),\n    epochs=5,\n    generations=3,\n    input_size=10,\n    hidden_size=20,\n    output_size=3,\n    optimizer=optimizer\n)\n</code></pre> <p>This example trains a simple dense network for 5 epochs with 3 generations of evolution using the selected optimizer.</p>"},{"location":"page_examples/#2-training-a-convolutional-neural-network-cnn","title":"2. Training a Convolutional Neural Network (CNN)","text":"<p>For image-like data, a convolutional neural network can be used:</p> <pre><code># Generate synthetic image data\nx_conv_train = np.random.random((20, 10, 10, 1))\ny_conv_train = np.random.randint(3, size=(20,))\nx_conv_test = np.random.random((10, 10, 10, 1))\ny_conv_test = np.random.randint(3, size=(10,))\n\n# Train convolutional model\ntrained_cnn = gnn.trainer.train(\n    x_train=x_conv_train,\n    y_train=y_conv_train,\n    x_test=x_conv_test,\n    y_test=y_conv_test,\n    labels=range(3),\n    input_size=10,\n    hidden_size=10,\n    output_size=3,\n    input_shape=(10, 10, 1),\n    kernel_size=3,\n    optimizer=optimizer,\n    epochs=5,\n    generations=3\n)\n</code></pre>"},{"location":"page_examples/#3-training-with-monte-carlo-simulation","title":"3. Training with Monte Carlo Simulation","text":"<pre><code># Configure Monte Carlo Simulation\nsimulation_scheduler = gnn.structure.SimulationScheduler(\n    mode=gnn.structure.SimulationScheduler.PROGRESS_CHECK,\n    simulation_time=10,\n    simulation_epochs=2\n)\n\n# Train with Monte Carlo simulation\ntrained_model = gnn.trainer.train(\n    x_train=x_train,\n    y_train=y_train,\n    x_test=x_test,\n    y_test=y_test,\n    labels=range(3),\n    epochs=5,\n    generations=3,\n    input_size=10,\n    hidden_size=20,\n    output_size=3,\n    optimizer=optimizer,\n    simulation_scheduler=simulation_scheduler,\n    simulation_alg=gnn.montecarlo_alg\n)\n</code></pre> <p>This example enables Monte Carlo simulation during training to explore different evolutionary paths.</p>"},{"location":"page_github/","title":"Versions and GitHub","text":""},{"location":"page_github/#releases-overview","title":"Releases Overview","text":"<p>This project has undergone multiple releases, each marking a significant advancement in its development. Below is a summary of the key versions:</p>"},{"location":"page_github/#release-1-r1-2023","title":"Release 1 (R1) - 2023","text":"<ul> <li>The initial version of the project was built directly from the <code>master</code> branch.</li> <li>This version was developed as part of a thesis project and was not hosted in a GitHub repository.</li> <li>Provided the foundation for dynamic neural network adaptation and Monte Carlo Tree Search (MCTS)-based optimization.</li> </ul>"},{"location":"page_github/#release-2-r2-2024","title":"Release 2 (R2) - 2024","text":"<ul> <li>This version was derived from the codebase used for the ICCS 2024 paper:</li> <li>\"Dynamic Growing and Shrinking of Neural Networks with Monte Carlo Tree Search\"</li> <li>Published in Computational Science \u2013 ICCS 2024 (Springer LNCS, vol. 14832)</li> <li>Authors: Szymon \u015awiderski &amp; Agnieszka Jastrz\u0119bska</li> <li>First Online: June 28, 2024</li> <li>Pages: 362\u2013377</li> <li>Hosted on GitHub for improved version control and collaboration.</li> <li>Code improvements for performance and stability, incorporating refinements based on ICCS 2024 research findings.</li> </ul>"},{"location":"page_github/#release-3-r3-2025-ongoing","title":"Release 3 (R3) - 2025 (Ongoing)","text":"<ul> <li>Targeted for ICCS 2025 submission, building upon the advancements made in R2.</li> <li>Further optimizations and enhancements to the Monte Carlo Tree Search approach.</li> <li>Improvements in dynamic network restructuring and efficiency.</li> <li>Automated testing and performance monitoring integrations.</li> </ul>"},{"location":"page_github/#github-repository-enhancements","title":"GitHub Repository Enhancements","text":"<p>With the migration to GitHub, several automated processes were introduced to improve the development workflow:</p> <ul> <li>Automated Unit Tests: Each commit is automatically tested to ensure stability and correctness.</li> <li>Performance Timing and Optimization Checks: Continuous benchmarking to track computational efficiency.</li> <li>SonarQube Integration: Code quality and security analysis using SonarQube, ensuring maintainability and reducing technical debt.</li> <li>CI/CD Pipeline: Implementation of automated builds and testing workflows.</li> </ul>"},{"location":"page_github/#future-developments","title":"Future Developments","text":"<ul> <li>Continued refinements in neural network architecture adaptation.</li> <li>Exploration of additional reinforcement learning strategies for training efficiency.</li> <li>Further integration of AI-driven code quality and performance analysis tools.</li> </ul> <p>For more details and source code, visit the GitHub repository (link to be provided).</p>"},{"location":"page_layer/","title":"Layers","text":"<p>The <code>Layer</code> class is a core component in a neural network architecture, representing an individual computational layer within a larger model. Each layer is responsible for managing its weights, biases, and activation functions while orchestrating forward and backward propagation through its connections with other layers.</p> <p>This class supports various initialization strategies for weights and biases, dynamic layer connectivity, and parallel computation via threading to optimize training efficiency. Additionally, it ensures flexibility through customizable activation functions and optimization strategies. The class also provides utilities for managing input/output connections, detecting cyclic dependencies in the network, and deep-copying layers for advanced use cases.</p>"},{"location":"page_layer/#attributes","title":"Attributes","text":"<ul> <li><code>id</code>: A unique identifier for the layer.</li> <li><code>model</code>: A reference to the model the layer belongs to, enabling inter-layer operations.</li> <li><code>input_size</code>: The size (number of features) of the input expected by the layer.</li> <li><code>neurons</code>: The number of neurons in the layer.</li> <li><code>act_fun</code>: The activation function applied to the layer\u2019s output.</li> <li><code>is_ending</code>: A flag indicating if the layer is the last in the network.</li> <li><code>is_starting</code>: A flag indicating if the layer is the first in the network.</li> <li><code>input_layers_ids</code>: List of IDs of layers connected as inputs to this layer.</li> <li><code>output_layers_ids</code>: List of IDs of layers receiving outputs from this layer.</li> <li><code>f_input</code>: Stores the accumulated inputs for forward propagation.</li> <li><code>b_input</code>: Stores the accumulated inputs for backward propagation.</li> <li><code>reshspers</code>: A dictionary for reshaping parameters during propagation.</li> <li><code>optimizer</code>: The default optimizer for parameter updates (e.g., <code>SGDOptimizer</code>).</li> <li><code>optimizer_W</code>: A specific optimizer instance for weight updates.</li> <li><code>optimizer_B</code>: A specific optimizer instance for bias updates.</li> <li><code>connections</code>: A dictionary tracking connections between layers.</li> </ul>"},{"location":"page_layer/#initialization","title":"Initialization","text":"<p>The <code>__init__</code> method initializes a layer with the following parameters:</p> <ul> <li><code>_id</code>: A unique identifier for the layer.</li> <li><code>_model</code>: The model the layer is part of.</li> <li><code>input_size</code>: Size of the input vector.</li> <li><code>neurons</code>: Number of neurons in the layer.</li> <li><code>act_fun</code>: The activation function applied during forward propagation.</li> <li><code>layer_type</code> (optional): Specifies the type of layer initialization (e.g., <code>RANDOM</code>, <code>EYE</code>, <code>ZERO</code>). Defaults to <code>Layer_Type.RANDOM</code>.</li> <li><code>_optimizer</code> (optional): The optimizer for training weights and biases. Defaults to an instance of <code>SGDOptimizer</code>.</li> </ul>"},{"location":"page_layer/#core-functionalities","title":"Core Functionalities","text":""},{"location":"page_layer/#layer-connectivity","title":"Layer Connectivity","text":"<ul> <li> <p><code>set_as_ending()</code>   Marks the layer as the final layer in the network.</p> </li> <li> <p><code>set_as_starting()</code>   Marks the layer as the first layer in the network.</p> </li> <li> <p><code>connect_input(layer_id)</code>   Connects another layer as an input to this layer.</p> </li> <li> <p><code>connect_output(layer_id)</code>   Connects this layer as an output to another layer.</p> </li> <li> <p><code>disconnect(to_remove_layer_id)</code>   Removes the connection between this layer and a specified layer.</p> </li> </ul>"},{"location":"page_layer/#propagation","title":"Propagation","text":"<ul> <li> <p><code>forward_prop(X, sender_id, depth=0)</code>   Executes forward propagation through the layer. Accumulates input data from connected input layers, computes the activation values, and forwards them to connected output layers.</p> </li> <li> <p><code>back_prop(E, m, alpha)</code>   Executes backpropagation to compute and propagate errors. Updates weights and biases using gradients calculated from the error signal.</p> </li> </ul>"},{"location":"page_layer/#parameter-updates","title":"Parameter Updates","text":"<ul> <li><code>update_params(alpha)</code>   Updates the layer\u2019s weights (<code>W</code>) and biases (<code>B</code>) using the optimizer.</li> </ul>"},{"location":"page_layer/#static-utility-methods","title":"Static Utility Methods","text":"<ul> <li> <p><code>update_weights_shape(W, input_size)</code>   Adjusts the shape of the weight matrix to match the expected input size.</p> </li> <li> <p><code>calcuale_Z(W, I, B)</code>   Computes the weighted input (<code>Z</code>) for the layer.</p> </li> <li> <p><code>calcuale_dW(m, dZ, I)</code>   Calculates the gradient of weights (<code>dW</code>).</p> </li> <li> <p><code>calcuale_dB(m, dZ, B)</code>   Calculates the gradient of biases (<code>dB</code>).</p> </li> </ul>"},{"location":"page_layer/#thread-management","title":"Thread Management","text":"<ul> <li> <p><code>should_thread_forward()</code>   Determines if forward propagation should be executed in a separate thread based on active thread count and input readiness.</p> </li> <li> <p><code>should_thread_backward()</code>   Determines if backpropagation should be executed in a separate thread.</p> </li> </ul>"},{"location":"page_layer/#additional-utilities","title":"Additional Utilities","text":"<ul> <li> <p><code>get_all_childrens_connections(deepth=0)</code>   Retrieves all downstream connections from this layer.</p> </li> <li> <p><code>is_cyclic(visited, additional_pair, depth=0)</code>   Checks for cycles in the network starting from this layer.</p> </li> <li> <p><code>deepcopy()</code>   Creates a deep copy of the layer, duplicating all attributes except connections to the model.</p> </li> <li> <p><code>get_weights_summary()</code>   Returns a summary of the layer\u2019s weights and biases.</p> </li> <li> <p><code>get_paint_label()</code>   Provides a formatted string label for the layer, displaying its ID and connection dimensions.</p> </li> </ul>"},{"location":"page_layer/#string-representation","title":"String Representation","text":"<ul> <li><code>__str__()</code>   Returns a string representation of the layer, including its ID, model reference, and connection counts.</li> </ul>"},{"location":"page_layer/#example-usage","title":"Example Usage","text":"<pre><code># Creating a layer\nlayer = Layer(\n    _id=1, \n    _model=model_instance, \n    input_size=128, \n    neurons=64, \n    act_fun=ReLU(), \n    layer_type=Layer_Type.RANDOM, \n    _optimizer=AdamOptimizer()\n)\n\n# Connecting layers\nlayer.connect_input(0)\nlayer.connect_output(2)\n\n# Forward propagation\ninput_data = np.random.rand(128, 1)\nlayer.forward_prop(input_data, sender_id=0)\n\n# Backpropagation\nerror = np.random.rand(64, 1)\nlayer.back_prop(E=error, m=1, alpha=0.01)\n\n# Checking for cycles\nif layer.is_cyclic([], additional_pair=(0, 2)):\n    print(\"Cycle detected!\")\n</code></pre> <p>Let me know if you need further refinements or additional examples!</p>"},{"location":"page_layerconv/","title":"Convolutional Layers","text":""},{"location":"page_layerconv/#overview","title":"Overview","text":"<p>The <code>Conv</code> class represents a convolutional layer in a neural network, responsible for extracting spatial features from input data using convolution operations. It supports multiple activation functions, various weight initialization distributions, and backpropagation for training.</p>"},{"location":"page_layerconv/#class-definition","title":"Class Definition","text":"<pre><code>class Conv(Layer):\n</code></pre> <p>This class inherits from <code>Layer</code> and implements forward and backward propagation for convolutional operations.</p>"},{"location":"page_layerconv/#initialization","title":"Initialization","text":"<pre><code>__init__(self, _id, _model, input_shape, kernel_size, depth, act_fun, _optimizer=SGDOptimizer())\n</code></pre>"},{"location":"page_layerconv/#parameters","title":"Parameters:","text":"<ul> <li><code>_id</code>: Unique identifier for the layer.</li> <li><code>_model</code>: Reference to the neural network model.</li> <li><code>input_shape</code>: Tuple specifying the shape of the input (height, width, depth).</li> <li><code>kernel_size</code>: Size of the convolution kernel.</li> <li><code>depth</code>: Number of filters in the layer.</li> <li><code>act_fun</code>: Activation function used after convolution.</li> <li><code>_optimizer</code>: Optimization algorithm for updating weights (default: <code>SGDOptimizer</code>).</li> </ul>"},{"location":"page_layerconv/#attributes","title":"Attributes:","text":"<ul> <li><code>kernels</code>: Weight matrices for convolution.</li> <li><code>biases</code>: Bias terms for each filter.</li> <li><code>output_shape</code>: Shape of the output feature map.</li> <li><code>input_layers_ids</code>: List of input layer IDs.</li> <li><code>output_layers_ids</code>: List of output layer IDs.</li> <li><code>optimizer</code>: Optimization function for weight updates.</li> </ul>"},{"location":"page_layerconv/#forward-propagation","title":"Forward Propagation","text":"<pre><code>forward_prop(self, X, sender_id, deepth=0)\n</code></pre> <p>Performs convolution on the input <code>X</code> and applies the activation function.</p>"},{"location":"page_layerconv/#steps","title":"Steps:","text":"<ol> <li>Receives input from connected layers.</li> <li>Performs 2D convolution using <code>correlate2d</code>.</li> <li>Applies activation function to the output.</li> <li>Sends transformed output to the next layers.</li> </ol>"},{"location":"page_layerconv/#backward-propagation","title":"Backward Propagation","text":"<pre><code>back_prop(self, E, m, alpha)\n</code></pre> <p>Computes gradients and updates weights during training.</p>"},{"location":"page_layerconv/#steps_1","title":"Steps:","text":"<ol> <li>Reshapes error gradient <code>E</code> to match output shape.</li> <li>Computes the gradient of the activation function.</li> <li>Computes weight updates using the convolution operation.</li> <li>Propagates errors to previous layers.</li> <li>Updates kernels and biases using the optimizer.</li> </ol>"},{"location":"page_layerconv/#weight-initialization","title":"Weight Initialization","text":"<p>Weights and biases can be initialized using different distributions:</p> <ul> <li>Uniform: Values between -1 and 1.</li> <li>Normal: Mean 0, standard deviation 1/3.</li> <li>Gamma: Gamma distribution values.</li> <li>Reversed Gaussian: Custom reversed normal distribution.</li> </ul> <pre><code>if WEIGHT_DISTRIBUTION_MODE == DistributionMode.UNIFORM:\n    self.kernels = np.random.uniform(-1.0, 1.0, self.kernels_shape) - 0.5\n</code></pre>"},{"location":"page_layerconv/#utility-methods","title":"Utility Methods","text":"<ul> <li><code>get_output_size()</code>: Returns the flattened output size.</li> <li><code>deepcopy()</code>: Creates a deep copy of the layer.</li> <li><code>update_params(alpha)</code>: Updates kernel weights and biases.</li> <li><code>get_weights_summary()</code>: Returns mean values of kernels and biases.</li> </ul>"},{"location":"page_layerconv/#example-usage","title":"Example Usage","text":"<pre><code>conv_layer = Conv(\n    _id=1,\n    _model=model,\n    input_shape=(28, 28, 1),\n    kernel_size=3,\n    depth=16,\n    act_fun=ReLU(),\n    _optimizer=AdamOptimizer()\n)\n</code></pre> <p>This creates a convolutional layer with 16 filters of size 3x3, using ReLU activation and Adam optimization.</p>"},{"location":"page_layerconv/#summary","title":"Summary","text":"<p>The <code>Conv</code> class implements a convolutional layer with customizable initialization, activation functions, and learning algorithms. It integrates into a neural network model, supporting both forward and backward propagation for training.</p>"},{"location":"page_model/","title":"Model","text":""},{"location":"page_model/#overview","title":"Overview","text":"<p>The <code>Model</code> class implements a flexible neural network model with support for both feed-forward and convolutional architectures. It is designed to allow for dynamic creation of layers, forward and backward propagation, and the training process using gradient descent. The model supports multiple types of layers, including regular layers and convolutional layers, and provides various functionalities such as layer management, loss calculation, and accuracy evaluation.</p>"},{"location":"page_model/#constructor","title":"Constructor","text":"<pre><code>class Model:\n    def __init__(self, input_size, hidden_size, output_size, loss_function = Loss.multiclass_cross_entropy, activation_fun = Activations.Sigmoid, input_paths = 1, _optimizer = SGDOptimizer()):\n</code></pre>"},{"location":"page_model/#parameters","title":"Parameters:","text":"<ul> <li><code>input_size</code>: Integer specifying the size of the input layer.</li> <li><code>hidden_size</code>: Integer specifying the size of hidden layers.</li> <li><code>output_size</code>: Integer specifying the size of the output layer.</li> <li><code>loss_function</code>: The loss function used for training. Default is <code>Loss.multiclass_cross_entropy</code>.</li> <li><code>activation_fun</code>: The activation function used in the layers. Default is <code>Activations.Sigmoid</code>.</li> <li><code>input_paths</code>: Number of input paths. Default is <code>1</code>.</li> <li><code>_optimizer</code>: The optimizer used for training. Default is <code>SGDOptimizer()</code>.</li> </ul>"},{"location":"page_model/#key-attributes","title":"Key Attributes:","text":"<ul> <li><code>batch_size</code>: The batch size used during training (default is 128).</li> <li><code>loss_function</code>: The loss function used for training.</li> <li><code>input_size</code>: The size of the input layer.</li> <li><code>hidden_size</code>: The size of the hidden layers.</li> <li><code>output_size</code>: The size of the output layer.</li> <li><code>hidden_layers</code>: A list of hidden layers in the model.</li> <li><code>avaible_id</code>: An identifier used for new layers.</li> <li><code>activation_fun</code>: The activation function used in layers.</li> <li><code>input_layers</code>: A list of input layers.</li> <li><code>optimizer</code>: The optimizer used for training.</li> <li><code>output_layer</code>: The output layer of the model.</li> </ul>"},{"location":"page_model/#methods","title":"Methods","text":""},{"location":"page_model/#set_convolution_modeinput_shape-kernel_size-depth","title":"<code>set_convolution_mode(input_shape, kernel_size, depth)</code>","text":"<p>Sets the model to use convolutional layers.</p> <ul> <li><code>input_shape</code>: Shape of the input for convolutional layers.</li> <li><code>kernel_size</code>: The size of the convolutional kernels.</li> <li><code>depth</code>: The number of convolutional kernels.</li> </ul> <p>This method converts the input layers to convolutional layers and adjusts their connections.</p>"},{"location":"page_model/#add_res_layerlayer_from_id-layer_to_id-layer_type-layer_typezero","title":"<code>add_res_layer(layer_from_id, layer_to_id, layer_type = Layer_Type.ZERO)</code>","text":"<p>Adds a residual layer between two existing layers. This helps to improve training by allowing the model to learn residual functions.</p> <ul> <li><code>layer_from_id</code>: The ID of the starting layer.</li> <li><code>layer_to_id</code>: The ID of the ending layer.</li> <li><code>layer_type</code>: Type of the residual layer (default is <code>Layer_Type.ZERO</code>).</li> </ul> <p>Returns the ID of the new residual layer.</p>"},{"location":"page_model/#add_norm_layerlayer_from_id-layer_to_id-layer_type-layer_typerandom","title":"<code>add_norm_layer(layer_from_id, layer_to_id, layer_type = Layer_Type.RANDOM)</code>","text":"<p>Adds a normalization layer between two layers. This helps with the stability and performance of the training process.</p> <ul> <li><code>layer_from_id</code>: The ID of the starting layer.</li> <li><code>layer_to_id</code>: The ID of the ending layer.</li> <li><code>layer_type</code>: Type of normalization layer (default is <code>Layer_Type.RANDOM</code>).</li> </ul> <p>Returns the ID of the new normalization layer.</p>"},{"location":"page_model/#add_sequential_output_layer","title":"<code>add_sequential_output_Layer()</code>","text":"<p>Adds a sequential output layer to the model. This is typically used when the model has multiple output paths.</p> <p>Returns the ID of the newly added sequential output layer.</p>"},{"location":"page_model/#add_conv_norm_layerlayer_from_id-layer_to_id-layer_type-layer_typerandom","title":"<code>add_conv_norm_layer(layer_from_id, layer_to_id, layer_type = Layer_Type.RANDOM)</code>","text":"<p>Adds a convolutional normalization layer between two layers.</p> <ul> <li><code>layer_from_id</code>: The ID of the starting layer.</li> <li><code>layer_to_id</code>: The ID of the ending layer.</li> <li><code>layer_type</code>: Type of the normalization layer (default is <code>Layer_Type.RANDOM</code>).</li> </ul> <p>Returns the ID of the newly added convolutional normalization layer.</p>"},{"location":"page_model/#add_conv_res_layerlayer_from_id-layer_to_id-layer_type-layer_typezero","title":"<code>add_conv_res_layer(layer_from_id, layer_to_id, layer_type = Layer_Type.ZERO)</code>","text":"<p>Adds a convolutional residual layer between two layers.</p> <ul> <li><code>layer_from_id</code>: The ID of the starting layer.</li> <li><code>layer_to_id</code>: The ID of the ending layer.</li> <li><code>layer_type</code>: Type of the residual layer (default is <code>Layer_Type.ZERO</code>).</li> </ul> <p>Returns the ID of the newly added convolutional residual layer.</p>"},{"location":"page_model/#add_connectionlayer_from_id-layer_to_id","title":"<code>add_connection(layer_from_id, layer_to_id)</code>","text":"<p>Adds a connection between two layers.</p> <ul> <li><code>layer_from_id</code>: The ID of the starting layer.</li> <li><code>layer_to_id</code>: The ID of the ending layer.</li> </ul>"},{"location":"page_model/#get_layerid","title":"<code>get_layer(id)</code>","text":"<p>Returns the layer with the specified ID.</p>"},{"location":"page_model/#get_predictionsa2","title":"<code>get_predictions(A2)</code>","text":"<p>Returns the predicted class for each sample.</p> <ul> <li><code>A2</code>: The output of the model's forward pass.</li> </ul>"},{"location":"page_model/#get_accuracypredictions-y","title":"<code>get_accuracy(predictions, Y)</code>","text":"<p>Calculates the accuracy of the model by comparing the predictions to the true values.</p> <ul> <li><code>predictions</code>: The predicted output.</li> <li><code>Y</code>: The true output.</li> </ul> <p>Returns the accuracy as a float.</p>"},{"location":"page_model/#forward_propinput","title":"<code>forward_prop(input)</code>","text":"<p>Performs forward propagation through the network, computing the output for the given input.</p> <ul> <li><code>input</code>: The input data to pass through the network.</li> </ul> <p>Returns the output of the network after forward propagation.</p>"},{"location":"page_model/#back_prope-m-alpha","title":"<code>back_prop(E, m, alpha)</code>","text":"<p>Performs backpropagation to adjust the weights based on the error.</p> <ul> <li><code>E</code>: The error signal.</li> <li><code>m</code>: The number of samples in the batch.</li> <li><code>alpha</code>: The learning rate.</li> </ul>"},{"location":"page_model/#gradient_descentx-y-iterations-lr_scheduler-quiet-false-one_hot_needed-true-path","title":"<code>gradient_descent(X, Y, iterations, lr_scheduler, quiet = False, one_hot_needed = True, path=\".\")</code>","text":"<p>Trains the model using gradient descent.</p> <ul> <li><code>X</code>: The input data.</li> <li><code>Y</code>: The true output.</li> <li><code>iterations</code>: The number of iterations for training.</li> <li><code>lr_scheduler</code>: A scheduler for adjusting the learning rate.</li> <li><code>quiet</code>: Whether to suppress the output during training (default is <code>False</code>).</li> <li><code>one_hot_needed</code>: Whether the output is one-hot encoded (default is <code>True</code>).</li> <li><code>path</code>: The path where the model should be saved.</li> </ul> <p>Returns the accuracy history for the last iteration.</p>"},{"location":"page_model/#evaluatex-y","title":"<code>evaluate(x, y)</code>","text":"<p>Evaluates the performance of the model on the given dataset.</p> <ul> <li><code>x</code>: The input data.</li> <li><code>y</code>: The true output.</li> </ul> <p>Returns the accuracy of the model.</p>"},{"location":"page_model/#remove_layerlayer_id-preserve_flow-true","title":"<code>remove_layer(layer_id, preserve_flow = True)</code>","text":"<p>Removes a layer from the model and adjusts the connections accordingly.</p> <ul> <li><code>layer_id</code>: The ID of the layer to remove.</li> <li><code>preserve_flow</code>: Whether to preserve the flow of connections (default is <code>True</code>).</li> </ul>"},{"location":"page_model/#get_all_childrens_connections","title":"<code>get_all_childrens_connections()</code>","text":"<p>Returns a list of all connections in the model.</p>"},{"location":"page_model/#deepcopy","title":"<code>deepcopy()</code>","text":"<p>Creates a deep copy of the model.</p> <p>Returns the copied model.</p>"},{"location":"page_model/#is_cyclicadditional_pair","title":"<code>is_cyclic(additional_pair)</code>","text":"<p>Checks if adding a new connection would create a cycle in the network.</p> <ul> <li><code>additional_pair</code>: A tuple representing the new connection.</li> </ul> <p>Returns <code>True</code> if a cycle is detected, otherwise <code>False</code>.</p>"},{"location":"page_model/#get_sequence_connection","title":"<code>get_sequence_connection()</code>","text":"<p>Returns a list of all layer connections in sequence.</p>"},{"location":"page_model/#get_state_representation","title":"<code>get_state_representation()</code>","text":"<p>Returns a string representation of the model's current state.</p>"},{"location":"page_model/#show_connection_table","title":"<code>show_connection_table()</code>","text":"<p>Prints a table showing the layers and their connections.</p>"},{"location":"page_model/#get_connection_table","title":"<code>get_connection_table()</code>","text":"<p>Returns a string table showing the layers and their connections.</p>"},{"location":"page_model/#example-usage","title":"Example Usage","text":"<pre><code># Initialize the model\nmodel = Model(input_size=28*28, hidden_size=128, output_size=10)\n\n# Train the model\nhistory = model.gradient_descent(X_train, Y_train, iterations=10, lr_scheduler=my_lr_scheduler)\n\n# Evaluate the model\naccuracy = model.evaluate(X_test, Y_test)\n</code></pre>"},{"location":"page_model/#notes","title":"Notes","text":"<ul> <li>The model supports both feed-forward and convolutional architectures.</li> <li>It supports dynamic layer creation, allowing for easy experimentation with different network structures.</li> <li>Various utility functions for training, evaluation, and model management are provided.</li> </ul>"},{"location":"page_train/","title":"Training Function","text":""},{"location":"page_train/#overview","title":"Overview","text":"<p>The <code>train</code> function is responsible for training a neural network model using gradient descent while incorporating simulation-based optimizations. The function handles data preprocessing, model initialization, training, and simulation-driven improvements.</p>"},{"location":"page_train/#function-signature","title":"Function Signature","text":"<pre><code>train(x_train, x_test, y_train, y_test, labels, path, model_name, epochs, generations, input_size, hidden_size, output_size, input_shape, kernel_size, deepth, batch_size=128, simulation_set_size=20, simulation_alg=montecarlo_alg, sim_set_generator=create_simulation_set_SAMLE, simulation_scheduler=SimulationScheduler(SimulationScheduler.PROGRESS_CHECK, simulation_time=60, simulation_epochs=20), lr_scheduler=LearningRateScheduler(LearningRateScheduler.PROGRESIVE, 0.03, 0.8), loss_function=Loss.multiclass_cross_entropy, activation_fun=Activations.Sigmoid, input_paths=1, sample_sub_generator=None, simulation_score=Simulation_score(), optimizer=SGDOptimizer())\n</code></pre>"},{"location":"page_train/#parameters","title":"Parameters","text":"Parameter Type Description <code>x_train</code> array-like Training feature data <code>x_test</code> array-like Testing feature data <code>y_train</code> array-like Training labels <code>y_test</code> array-like Testing labels <code>labels</code> list List of label names <code>path</code> str Directory path for saving model and history <code>model_name</code> str Name of the model to be saved <code>epochs</code> int Number of epochs for each training phase <code>generations</code> int Number of training iterations with simulations <code>input_size</code> int Number of input neurons <code>hidden_size</code> int Number of hidden neurons <code>output_size</code> int Number of output neurons <code>input_shape</code> tuple or None Shape of input data (for convolutional mode) <code>kernel_size</code> int Size of convolution kernel <code>deepth</code> int Depth of convolution layers <code>batch_size</code> int Batch size for training (default: 128) <code>simulation_set_size</code> int Number of samples used in simulation (default: 20) <code>simulation_alg</code> object Algorithm used for simulations (default: <code>montecarlo_alg</code>) <code>sim_set_generator</code> function Function for generating simulation set <code>simulation_scheduler</code> object Scheduler controlling simulation frequency <code>lr_scheduler</code> object Learning rate scheduler <code>loss_function</code> function Loss function used during training <code>activation_fun</code> function Activation function used in the model <code>input_paths</code> int Number of input paths for model <code>sample_sub_generator</code> function or None Function for generating sample subsets (default: None) <code>simulation_score</code> object Scoring function for simulations <code>optimizer</code> object Optimizer used for gradient descent (default: <code>SGDOptimizer</code>)"},{"location":"page_train/#returns","title":"Returns","text":"<ul> <li>A trained <code>Model</code> instance after applying training and simulation steps.</li> </ul>"},{"location":"page_train/#example-usage","title":"Example Usage","text":""},{"location":"page_train/#basic-training-example","title":"Basic Training Example","text":"<pre><code>from training_module import train\n\n# Sample data (replace with actual dataset)\nx_train = [[0.1, 0.2], [0.3, 0.4]]\nx_test = [[0.5, 0.6]]\ny_train = [0, 1]\ny_test = [1]\nlabels = ['class_0', 'class_1']\n\n# Define parameters\npath = \"./model_output/\"\nmodel_name = \"neural_net\"\nepochs = 10\ngenerations = 5\ninput_size = 2\nhidden_size = 4\noutput_size = 2\ninput_shape = None\nkernel_size = 3\ndeepth = 2\n\n# Train model\nmodel = train(x_train, x_test, y_train, y_test, labels, path, model_name, epochs, generations, input_size, hidden_size, output_size, input_shape, kernel_size, deepth)\n</code></pre>"},{"location":"page_train/#training-with-custom-learning-rate-scheduler","title":"Training with Custom Learning Rate Scheduler","text":"<pre><code>from training_module import train, LearningRateScheduler\n\nlr_scheduler = LearningRateScheduler(LearningRateScheduler.EXPONENTIAL, 0.05, 0.9)\n\nmodel = train(x_train, x_test, y_train, y_test, labels, path, model_name, epochs, generations, input_size, hidden_size, output_size, input_shape, kernel_size, deepth, lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"page_train/#training-with-simulation-algorithm","title":"Training with Simulation Algorithm","text":"<pre><code>from training_module import train, montecarlo_alg, create_simulation_set_SAMLE\n\nsim_alg = montecarlo_alg\nsim_set_gen = create_simulation_set_SAMLE\n\nmodel = train(x_train, x_test, y_train, y_test, labels, path, model_name, epochs, generations, input_size, hidden_size, output_size, input_shape, kernel_size, deepth, simulation_alg=sim_alg, sim_set_generator=sim_set_gen)\n</code></pre>"},{"location":"page_train/#notes","title":"Notes","text":"<ul> <li>Ensure that the dataset is properly formatted before passing it to the <code>train</code> function.</li> <li>The function includes a simulation-based improvement mechanism that optimizes model performance through iterative refinements.</li> <li>Various parameters such as <code>simulation_alg</code>, <code>lr_scheduler</code>, and <code>optimizer</code> allow customization of the training process.</li> </ul>"},{"location":"page_train/#references","title":"References","text":"<ul> <li>Simulation Scheduler: Used to control the timing and execution of simulations.</li> <li>Learning Rate Scheduler: Defines the learning rate adaptation strategy.</li> <li>Simulation Algorithms: Improve model performance through reinforcement learning-based exploration.</li> </ul>"}]}